''' Dataset csv Ledger file generator script

This script generates a csv ledger file for a given classification dataset. This csv file would be used when loading a dataset.
'''

"""
Generate Dataset CSV Ledger Script.

This script is designed to facilitate the creation of a CSV file that acts as a ledger for custom event-based datasets. The CSV file generated by this script contains paths to individual dataset samples along with their corresponding classification labels. This utility is particularly useful for new users or researchers looking to integrate custom datasets into their event-based vision projects.

The script supports specifying various dataset properties such as the name, path, and whether the dataset is split into training/testing sets. It accommodates datasets stored in different formats by allowing the user to manually specify the input format.

Usage:
    The script is run from the command line, accepting arguments to specify the dataset path, name, and other options related to data splits and output formatting.

    Example command:
    ```
    python gen_dataset_csv.py --dataset_path /path/to/dataset --dataset_name MyDataset --data_split
    ```

Arguments:
    -d, --dataset_path : The path to the root directory of the dataset.
    -n, --dataset_name : The name of the dataset, which defines the input format (.txt, .csv) for custom classification datasets.
    -s, --data_split   : A flag indicating whether the dataset contains separate splits (e.g., train/test).
    -x, --split_name   : The name of the split for which to generate the ledger. Defaults to 'train'.

Functionality:
    The script performs the following steps:
    1. Validates the provided dataset path and name.
    2. Determines the appropriate format and structure of the dataset based on the provided information.
    3. Scans the dataset directory to locate all event files.
    4. Maps each event file to its classification label based on the directory structure.
    5. Generates a CSV file listing the relative paths to the event files and their labels.

Output:
    A CSV file is created in the dataset's root directory (or a specified output path) listing the relative paths to event files along with their classification labels. This ledger file is essential for loading and processing the dataset in machine learning workflows.

Note:
    This script is intended to be modified and extended as needed to support new dataset formats and structures. Users are encouraged to adjust the parsing and labeling logic to fit their specific dataset requirements.
"""


import os
import argparse
from utils import get_dataset_name
from utils import get_event_files

# main function that reads arguments provided by the user and acts accordingly
def main():
    # read input arguments
    args = parse_args()

    # Check if dataset path and dataset name/type provided. Otherwise script terminates.
    if args["dataset"] and args["dataset_name"]:    
        # get dataset path
        dataset_path = args["dataset"]

        # make sure dataset path ends with separation
        dataset_path = os.path.join(dataset_path, "")

        # convert to Linux dir format
        dataset_path = dataset_path.replace("\\", '/')

        # get dataset name which infers the format it was stored in or manually specify format (.'txt', '.csv')
        dataset_type = args["dataset_name"]

        # does the data contain splits?
        data_split_flag = args["data_split"]

        # get split name if specified
        split_name = args["split_name"]

        # get dataset name from given directory
        dataset_name = get_dataset_name(dataset_path)

        # set output path to be the dataset's root
        output_path = dataset_path

        print("Generating csv ledger file for dataset:", dataset_name)

        if data_split_flag:
            print("split:", split_name)

        else:
            print("split: none")
            # remove split if split_flag is False
            split_name = ''

        # generate a csv ledger file 
        gen_csv_file(dataset_path, dataset_name, dataset_type, data_split_flag, output_path, split_name)

# argument parser functions to load model and choose the generator settings
def parse_args():
    # construct the argument parser and parse the arguments
    ap = argparse.ArgumentParser()

    ap.add_argument("-d", "--dataset_path", required=True,
        help="path to the dataset")

    ap.add_argument("-n", "--dataset_name", required=True,
        help="dataset name which defines the input format. Can set as '.txt' or '.csv' for custom classification datasets")

    ap.add_argument("-s", "--data_split", required=False, default=False,
        action="store_true", help="flag to inform that the data is split (train/test)")

    ap.add_argument("-x", "--split_name", required=False, default='train',
        help="flag to specify the split name to generate the ledger for. defaults to 'train'")

    args = vars(ap.parse_args())
    return args

# function to generate the .csv ledger file which includes <path to events file, label>
def gen_csv_file(dataset_path, dataset_name, dataset_type, data_split_flag, output_path="", split='train'):
    if output_path == "":
        output_path = dataset_path

    # get paths to every event file in the specified dataset + split
    event_files = get_event_files(dataset_path, dataset_type, split)

    # find total number of files
    total_files = len(event_files)
    print("Total dataset's event files:", total_files)

    # generate csv ledger file name
    csv_file_name = dataset_name + '_' + split + '.csv' if data_split_flag is True else dataset_name + '.csv'

    with open(os.path.join(output_path, csv_file_name), "w+") as csv_file:
        # add column headers
        csv_file.write("events_file_path,class_index\n")

        # convert paths from absolute to relative paths to the root: root/<split>/class/event_file
        if data_split_flag:
            for i, event_file in enumerate(event_files):
                print("processing file [{}/{}]".format(str(i+1), str(total_files)), end='\r')
                # convert to Linux dir format & get relative path
                file_path = "/".join(event_file.replace('\\', '/').split('/')[-3:])
                # note: replace with [-4:] to include root as well.

                # get classification for the given file
                classification = event_file.replace('\\', '/').split('/')[-2]

                # convert classification to int label
                class_labels = gen_labels_index(dataset_type)

                # get class index for classification
                label = class_labels[classification]

                # write data to ledger file
                csv_file.write(file_path + ',' + str(label) + '\n')
        else:
            for i, event_file in enumerate(event_files):
                print("processing file [{}/{}]".format(str(i+1), str(total_files)), end='\r') 
                # convert to Linux dir format
                file_path = "/".join(event_file.replace('\\', '/').split('/')[-2:])
                # note: replace with [-3:] to include root as well.

                # get classification for the given file
                classification = event_file.replace('\\', '/').split('/')[-2]

                # convert classification to int label
                class_labels = gen_labels_index(dataset_type)

                # get class index for classification
                label = class_labels[classification]

                # write data to ledger file
                csv_file.write(file_path + ',' + str(label) + '\n')
        print()

# helper function to generate a look-up table using a dict to provide a int label for the class
def gen_labels_index(dataset_name):
    # class indices look up table for converting a class label to class index
    class_indices = {}
    # 1) N-MNIST or N-Caltech101
    if dataset_name == "N-MNIST":
        class_labels = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']

    elif dataset_name == "N-Caltech101":
        class_labels = ['accordion', 'airplanes', 'anchor', 'ant', 'BACKGROUND_Google', 'barrel', 'bass', 
            'beaver', 'binocular', 'bonsai', 'brain', 'brontosaurus', 'buddha', 'butterfly', 'camera', 'cannon', 
            'car_side', 'ceiling_fan', 'cellphone', 'chair', 'chandelier', 'cougar_body', 'cougar_face', 'crab', 
            'crayfish', 'crocodile', 'crocodile_head', 'cup', 'dalmatian', 'dollar_bill', 'dolphin', 'dragonfly', 
            'electric_guitar', 'elephant', 'emu', 'euphonium', 'ewer', 'Faces_easy', 'ferry', 'flamingo', 
            'flamingo_head', 'garfield', 'gerenuk', 'gramophone', 'grand_piano', 'hawksbill', 'headphone', 
            'hedgehog', 'helicopter', 'ibis', 'inline_skate', 'joshua_tree', 'kangaroo', 'ketch', 'lamp', 'laptop', 
            'Leopards', 'llama', 'lobster', 'lotus', 'mandolin', 'mayfly', 'menorah', 'metronome', 'minaret', 'Motorbikes', 
            'nautilus', 'octopus', 'okapi', 'pagoda', 'panda', 'pigeon', 'pizza', 'platypus', 'pyramid', 'revolver', 
            'rhino', 'rooster', 'saxophone', 'schooner', 'scissors', 'scorpion', 'sea_horse', 'snoopy', 'soccer_ball', 
            'stapler', 'starfish', 'stegosaurus', 'stop_sign', 'strawberry', 'sunflower', 'tick', 'trilobite', 'umbrella', 
            'watch', 'water_lilly', 'wheelchair', 'wild_cat', 'windsor_chair', 'wrench', 'yin_yang']

    # 3) N-Cars
    elif dataset_name == "N-Cars":
        class_labels = ['background', 'cars']

    # 4) CIFAR10-DVS
    elif dataset_name == "CIFAR10-DVS":
        class_labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']

    # 5) ASL-DVS
    elif dataset_name == "ASL-DVS":
        class_labels =  ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y']

    # 6) DVS-Gesture
    elif dataset_name == "DVS-Gesture":
        # class_labels = ['hand_clapping', 'right_hand_wave', 'left_hand_wave', 'right_arm_clockwise', 'right_arm_counter_clockwise', 
        #                 'left_arm_clockwise', 'left_arm_counter_clockwise', 'arm_roll', 'air_drums', 'air_guitar', 'other_gestures']
        class_labels = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11']

    # NOTE: define other datasets here as needed.

    # Otherwise, dataset is not supported
    else:
        raise ValueError('dataset {} not supported!'.format(dataset_name))
    
    # populate lookup table with index for each class label
    class_indices = {label: index for (index, label) in enumerate(class_labels)}

    return class_indices
   
if __name__ == "__main__":
    main()